# 부하 테스트 분석 (WIP)

## 목차

- [I. Setting](#i-setting)
  - [1. About Target Application](#1-about-target-application)
  - [2. Infrastructure](#2-infrastructure)
- [II. 테스트 시나리오 소개](#ii-테스트-시나리오-소개)
  - [1. 테스트 목적](#1-테스트-목적)
  - [2. 테스트 시나리오 구성](#2-테스트-시나리오-구성)
  - [3. 주요 테스트 시나리오](#3-주요-테스트-시나리오)
  - [4. 성능 메트릭 및 임계값](#4-성능-메트릭-및-임계값)
  - [5. 핵심 테스트 포인트](#5-핵심-테스트-포인트)
- [III. 테스트 결과](#iii-테스트-결과)
- [IV. 병목 진단](#iv-병목-진단)
- [V. 병목 원인 추정](#v-병목-원인-추정)
- [VI. 해결 방안](#vi-해결-방안)
- [VII. Lesson Learnt](#vii-lesson-learnt)

---

## I. Setting

설정 관련 부분을 기술하는 현 챕터는 가독성을 위해 명사형 어미로 작성 했습니다.

### 1. About Target Application

- 공연 좌석 예매 서비스를 구현한 프로젝트
- 도메인 특성 상 발생할 것으로 예측되는 Peak Pattern 트래픽에 대비해 *대기열* 방식의 *유량 제어*를 구현한 상태
- 대기열은 DB를 활용하는 구현체와 Redis를 활용하는 구현체가 존재.
- 이번 부하 테스트에서는 *부하에 따른 결과가 보다 두드러지게 나타나도록* DB 대기열을 활용.

#### 1.1 타겟 API 소개

- **대기열 토큰 발급 API**: 서비스 진입을 위해 대기열 토큰 발급을 요청. `INSERT` 쿼리 발생
- **대기열 토큰 폴링 API**: 현재 자신의 대기 순번을 확인하기 위한 Polling 요청을 받는 API. `COUNT` 쿼리 발생

#### 1.2 애플리케이션 설정

**HikariCP DB Connection Pool**
- max: 10
- min_idle: 5
- 부하에 따른 병목이 두드러지도록 풀의 크기를 좁게 설정함

**Tomcat Thread Pool**
- 별도 설정 없음
- 기본값: 최대 200, 유휴 상태 8개 유지

**VM Option**
- max: 512MB
- min: 256MB
> 부하 테스트 도중 Grafana Dashboard 상에 표시되는 JVM 메모리 영역의 크기가 설정과 다름을 확인하였습니다.
> 
> 이와 관련할 것으로 추정되는 `docker-compose`에 명세된 컨테이너 실행 옵션, `Dockerfile`에 정의된 Java Application 실행 옵션, `application.yml` 에 정의된 옵션을 살펴보았으나 문제를 찾지 못했습니다.
> 
> 그렇다면 Java 의 버전 또는 실행 환경 이미지에 따라 기본적으로 적용된 옵션이 상기한 사용자 정의 옵션을 덮어썼을 가능성을 고려해보게 되었습니다.
>
> 이에 따라 조사해본 결과, Java 10+ 버전의 기본 옵션인 `-XX:+UseContainerSupport`로 인해 실제로는 256MB의 힙 크기를 호스트로부터 할당받게 된다는 사실을 이후 알게 되었습니다.
>
> 그 결과, 호스트 환경 내에서 실행되는 컨테이너에 대해 적용되는 메모리 할당 크기가 있고, JVM 이 이를 토대로 적정 비율(결과 상 90% 추정) 만큼의 메모리를 할당 받는 것이라는 결론에 도달했습니다.

### 2. Infrastructure

**EC2**
- 인스턴스 타입: t2.micro
- vCPU: 1
- RAM: 1GiB
- 스토리지: 20GB SSD

**RDS**
- 인스턴스 타입: db.t4g.micro
- vCPU: 2
- RAM: 1GiB
- 스토리지: 20GB SSD

---

## II. 테스트 시나리오 소개

### 1. 테스트 목적

공연 예약 시스템의 대기열(Queue) 기능에 대한 성능 및 부하 테스트를 수행하였습니다.

주요 목적은 다음과 같습니다.

- 대기열 토큰 발급 기능의 처리 성능 검증
- 대기열 상태 폴링(Polling) 시스템의 안정성 확인
- 높은 동시 접속 상황에서의 시스템 응답 시간 및 성공률 측정
- 실제 사용자 행동 패턴을 시뮬레이션한 현실적인 부하 테스트

테스트 스크립트 링크는 다음과 같습니다.
> [링크]()

### 2. 테스트 시행 도구 & 서버 상태 모니터링 도구 소개
- 부하기 : Grafana K6
- 모니터링 : Grafana
- 시계열 메트릭 데이터베이스 : Prometheus
- 메트릭 출처 : Spring Actuator 에 의해 공개된 Metric 조회 Endpoint

### 3. 테스트 시나리오 구성

#### 3.1 부하 프로파일 (Load Profile)

총 30분간 7단계로 구성된 점진적 부하 증가 패턴

| 단계 | 기간 | 목표 사용자 수 | 설명 |
|------|------|---------------|------|
| 1 | 2분 | 10명 | 워밍업 단계 |
| 2 | 3분 | 50명 | 초기 부하 증가 |
| 3 | 5분 | 100명 | 중간 부하 |
| 4 | 3분 | 200명 | 피크 부하 도달 |
| 5 | 10분 | 200명 | 피크 부하 유지 (스트레스 테스트) |
| 6 | 5분 | 50명 | 점진적 감소 |
| 7 | 2분 | 0명 | 종료 |

**핵심**: 10분간 200명의 동시 사용자를 유지하여 시스템의 지속적인 부하 처리 능력을 검증함.

#### 3.2 테스트 데이터 준비 (Setup Phase)

테스트 스크립트를 활용하여, 테스트 시작 전 다음 데이터를 자동으로 생성하도록 하였습니다.

- 5개의 공연 (BTS, IU, NewJeans, BLACKPINK, SEVENTEEN)
- 각 공연당 3~5개의 일정 생성
- 총 15~25개의 공연 일정 준비
- 예약 기간, 가격 등 실제와 유사한 데이터 설정

### 4. 주요 테스트 시나리오

각 가상 사용자(VU)는 다음 플로우를 반복 수행합니다.

**Step 1: 대기열 토큰 발급**
```
POST /api/queue/tokens
- userId와 scheduleId를 전송
- 대기열 토큰(tokenId) 발급받음
- 쿠키 형태로 토큰 저장
```

**Step 2: 대기열 상태 폴링**
```
GET /api/queue/status?scheduleId={scheduleId}
- 1~3초 간격으로 반복 폴링 (랜덤)
- 대기 인원 수(remaining) 확인
- 최대 20회까지 폴링 시도
```

**Step 3: 조기 종료 조건**
- 대기 인원이 0명이 되면 성공적으로 종료
- 대기 인원이 1,000명 초과 시 10% 확률로 포기 (실제 사용자 행동 반영)

### 5. 성능 메트릭 및 임계값

#### 5.1 커스텀 메트릭

| 메트릭 | 설명 | 임계값 |
|--------|------|--------|
| token_issue_success_rate | 토큰 발급 성공률 | 95% 이상 |
| queue_polling_success_rate | 폴링 성공률 | 98% 이상 |
| token_issue_duration | 토큰 발급 소요 시간 | 95%가 2초 미만 |
| queue_polling_duration | 폴링 응답 시간 | 95%가 1초 미만 |
| total_queue_requests | 총 대기열 요청 수 | 카운팅 |

#### 5.2 일반 HTTP 메트릭

| 메트릭 | 임계값 | 의미 |
|--------|--------|------|
| http_req_failed | 5% 미만 | HTTP 요청 실패율 |
| http_req_duration | 95%가 3초 미만 | 전체 요청 응답 시간 |

### 6. 핵심 테스트 포인트

#### 6.1 대기열 로직 검증
- 토큰 발급 → 폴링 → 순서의 전체 플로우 테스트

#### 6.2 실제 사용자 행동 반영
- 1~3초 랜덤 폴링 간격 (실제 사용자의 새로고침 패턴)
- 긴 대기 시 10% 포기율 (이탈 사용자 시뮬레이션)

#### 6.3 시스템 안정성
- 10분간 지속되는 피크 부하에서의 안정성 검증
- 메모리 누수, 커넥션 풀 고갈 등 문제 감지

---

## III. 테스트 결과

### k6 부하 테스트 결과

```javascript
2025-10-18 22:51:40      ✓ token issue status is 200
2025-10-18 22:51:40      ✓ token issue has tokenId
2025-10-18 22:51:40      ✓ token issue sets cookie
2025-10-18 22:51:40      ✓ queue polling status is 200
2025-10-18 22:51:40      ✓ queue polling returns number
2025-10-18 22:51:40 
2025-10-18 22:51:40      checks.........................: 100.00% 236104 out of 236104
2025-10-18 22:51:40      data_received..................: 25 MB   14 kB/s
2025-10-18 22:51:40      data_sent......................: 21 MB   12 kB/s
2025-10-18 22:51:40      http_req_blocked...............: avg=53.82µs   min=917ns     med=8.16µs    max=664.92ms    p(90)=18.16µs   p(95)=24.2µs   
2025-10-18 22:51:40      http_req_connecting............: avg=41.21µs   min=0s        med=0s        max=664.64ms    p(90)=0s        p(95)=0s       
2025-10-18 22:51:40    ✓ http_req_duration..............: avg=26.61ms   min=9.7ms     med=21.98ms   max=18.11s      p(90)=29.42ms   p(95)=32.91ms  
2025-10-18 22:51:40        { expected_response:true }...: avg=26.61ms   min=9.7ms     med=21.98ms   max=18.11s      p(90)=29.42ms   p(95)=32.91ms  
2025-10-18 22:51:40    ✓ http_req_failed................: 0.00%   0 out of 114738
2025-10-18 22:51:40      http_req_receiving.............: avg=112.39µs  min=7µs       med=79.25µs   max=21.37ms     p(90)=196.75µs  p(95)=270.04µs 
2025-10-18 22:51:40      http_req_sending...............: avg=200.37µs  min=2.83µs    med=30.5µs    max=18.09s      p(90)=65.29µs   p(95)=87.37µs  
2025-10-18 22:51:40      http_req_tls_handshaking.......: avg=0s        min=0s        med=0s        max=0s          p(90)=0s        p(95)=0s       
2025-10-18 22:51:40      http_req_waiting...............: avg=26.3ms    min=9.61ms    med=21.83ms   max=1.58s       p(90)=29.25ms   p(95)=32.73ms  
2025-10-18 22:51:40      http_reqs......................: 114738  62.617507/s
2025-10-18 22:51:40      iteration_duration.............: avg=32.82s    min=1.04s     med=39.76s    max=58.55s      p(90)=43.62s    p(95)=44.68s   
2025-10-18 22:51:40      iterations.....................: 6635    3.621007/s
2025-10-18 22:51:40    ✓ queue_polling_duration.........: avg=26.398065 min=9.705834  med=21.835938 max=18115.82337 p(90)=28.663917 p(95)=32.489916
2025-10-18 22:51:40    ✓ queue_polling_success_rate.....: 100.00% 108026 out of 108026
2025-10-18 22:51:40    ✓ token_issue_duration...........: avg=29.590183 min=15.799333 med=28.338813 max=1267.084334 p(90)=32.245438 p(95)=38.260425
2025-10-18 22:51:40    ✓ token_issue_success_rate.......: 100.00% 6684 out of 6684
2025-10-18 22:51:40      total_queue_requests...........: 114710  62.602226/s
2025-10-18 22:51:40      vus............................: 1       min=0                max=200
2025-10-18 22:51:40      vus_max........................: 200     min=200              max=200
```

### 그라파나 대시보드
<img width="1913" height="948" alt="부하테스트_전체_I:O관련" src="https://github.com/user-attachments/assets/76fe5dda-c65c-4a75-98eb-cc84cedc5f99" />

---

## IV. 병목 진단

### 발생한 문제 요약

22:40~22:45 구간에 다음 현상이 동시 발생함.

- HTTP 응답 시간 1.5초로 급증 (평균 6ms → 1500ms)
- timed-waiting 스레드 103개로 폭증 (이전 10개 → 103개)
- Major GC 발생 (190ms Stop-The-World)
- Minor GC 빈도 0.3 ops/s로 급증

### 상세 분석

#### 1. Thread States (스레드 상태)

**22:30 이전**
- runnable: 40
- timed-waiting: ~15

**22:45 시점**
- runnable: 40 (변화 없음)
- timed-waiting: 103 (급증)
- blocked: 0 (여전히 없음)

**문제 해석**

`timed-waiting` 103개는 대부분의 스레드가 대기 상태에 있음을 의미함. 이들이 대기하고 있을 가능성은 다음과 같음.

- **GC 완료 대기** (가장 가능성 높음)
- Database 커넥션 풀 대기
- 외부 API 응답 대기
- Thread Pool 작업 대기

#### 2. GC Collections (가비지 컬렉션 빈도)

**시간대별 변화**
- 22:25-22:30: Minor GC ~0.1 ops/s (정상)
- 22:35-22:40: Minor GC 0.3+ ops/s (3배 증가)

**문제 해석**

Minor GC 빈도 급증은 메모리 할당 속도가 급증했음을 의미함. Young Generation이 빠르게 차서 계속 수거가 발생함. 이는 객체 생성이 폭발적으로 증가했음을 나타냄.

**원인 추정**

부하 증가로 인한 객체 생성 폭증
- HTTP 요청 객체
- 대기열 토큰 객체
- 폴링 요청마다 생성되는 응답 객체
- 로그 객체 등

#### 3. Pause Durations (GC 일시정지)

**22:40 시점**
- Major GC 발생
- 일시정지 시간: ~190ms (Stop-The-World)

**핵심 원인**

Major GC 발생 과정은 다음과 같음.

1. Young Generation이 계속 차서 Minor GC 빈발
2. 일부 객체가 Old Generation으로 승격
3. Old Generation도 가득 참
4. **Major GC (Full GC) 트리거** (190ms STW 발생)
5. 모든 애플리케이션 스레드 일시정지
6. HTTP 요청들이 모두 대기 → 응답 시간 폭증

#### 4. Duration (HTTP 응답 시간)

**응답 시간 변화**
- 평균: 6.44ms
- 최대: 512ms (일반적)
- 22:45 스파이크: 1.5초

**1.5초까지 증가한 이유**

```
190ms (Major GC) + α (대기 시간) ≈ 1.5초

계산:
- GC 일시정지: 190ms
- GC 후 스레드 재시작 지연: ~100ms
- 대기열에 쌓인 요청 처리 지연: ~1000ms
- 총합: ~1.5초
```

### 근본 원인 (Root Cause)

**메모리 압박 → GC 압박 → 성능 저하**

```
부하 증가 (K6 테스트)
    ↓
객체 생성 폭증 (토큰, 폴링 응답 등)
    ↓
Young Gen 빠르게 가득 참
    ↓
Minor GC 빈도 급증 (0.3 ops/s)
    ↓
Old Gen으로 객체 승격
    ↓
Old Gen도 가득 참
    ↓
Major GC 발생 (190ms STW)
    ↓
모든 스레드 일시정지
    ↓
timed-waiting 103개로 급증
    ↓
HTTP 응답 시간 1.5초로 폭증
```

### timed-waiting 급증 원인

**핵심**: GC 중에는 모든 스레드가 멈춤

```java
// GC 발생 시
Thread 1: 요청 처리 중 → GC 대기 → timed-waiting
Thread 2: 요청 처리 중 → GC 대기 → timed-waiting
Thread 3: 요청 처리 중 → GC 대기 → timed-waiting
...
Thread 103: 모두 대기 상태

// GC 완료 후
Thread 1: 다시 runnable로 복귀
Thread 2: 다시 runnable로 복귀
...
```

따라서:
- `timed-waiting 103` = GC나 I/O 대기 중인 스레드
- `runnable 40` = 실제 작업 중인 스레드
- 총 143개 스레드 중 대부분이 대기 중

---

## V. 병목 원인 추정

### 1. GC 대기가 주요 원인

**timed-waiting 스레드 103개의 정체**

대부분의 스레드가 GC 완료를 대기하고 있었을 가능성이 가장 높음. Major GC 발생 시 190ms 동안 모든 애플리케이션 스레드가 일시정지되면서 요청 처리가 지연됨.

### 2. 객체 생성 폭증

**부하 증가로 인한 메모리 할당 급증**

다음과 같은 객체들이 폭발적으로 생성됨.

- HTTP 요청/응답 객체
- 대기열 토큰 객체
- 폴링 요청마다 생성되는 DTO 객체
- 로그 객체
- 데이터베이스 조회 결과 객체

### 3. Major GC 발생 메커니즘

**5단계 프로세스**

1. Young Generation이 계속 차서 Minor GC 빈발
2. 일부 객체가 Old Generation으로 승격
3. Old Generation도 가득 참
4. **Major GC (Full GC) 트리거** (190ms STW)
5. 모든 애플리케이션 스레드 일시정지

### 4. 응답 시간 급증 분석

**1.5초 응답 시간의 구성 요소**

```
총 1.5초 = GC 190ms + 재시작 지연 100ms + 처리 지연 1000ms
```

- **GC 일시정지**: 190ms
- **GC 후 스레드 재시작 지연**: ~100ms
- **대기열에 쌓인 요청 처리 지연**: ~1000ms

### 5. 메모리 부족의 근본 원인

**설정된 힙 크기의 한계**

- 설정: max 512MB, min 256MB
- 실제: Java 10+의 `-XX:+UseContainerSupport` 옵션으로 인해 256MB만 할당됨
- 200명의 동시 사용자 부하를 처리하기에는 부족한 메모리 크기

---

## VI. 해결 방안

### 1. 힙 메모리 증설

**우선순위: 최우선**

현재 256MB의 힙 크기는 200 VU 부하를 처리하기에 부족함.

**권장 조치**
```bash
# 컨테이너 환경에서 명시적으로 힙 크기 설정
-Xms512m -Xmx1024m

# 또는 컨테이너 메모리 비율 기반 설정
-XX:InitialRAMPercentage=50.0
-XX:MaxRAMPercentage=75.0
```

**기대 효과**
- Young Generation 크기 증가로 Minor GC 빈도 감소
- Old Generation 여유 공간 확보로 Major GC 발생 지연
- GC로 인한 STW 시간 감소

### 2. GC 알고리즘 최적화

**우선순위: 높음**

현재 사용 중인 GC 알고리즘 확인 후 G1GC 또는 ZGC 적용을 검토함.

**권장 조치**
```bash
# G1GC 적용 (응답 시간 개선)
-XX:+UseG1GC
-XX:MaxGCPauseMillis=200
-XX:G1HeapRegionSize=8m

# 또는 ZGC 적용 (매우 낮은 지연시간 요구 시)
-XX:+UseZGC
-XX:ZCollectionInterval=5
```

**기대 효과**
- STW 시간 최소화 (190ms → 50ms 이하)
- 응답 시간 스파이크 감소
- 처리량 향상

### 3. 객체 생성 최적화

**우선순위: 중간**

불필요한 객체 생성을 줄여 GC 압박을 완화함.

**권장 조치**
- 폴링 응답 DTO 객체 재사용 (Object Pool 패턴)
- StringBuilder 사용하여 문자열 연산 최적화
- 로깅 레벨 조정 (DEBUG → INFO)
- Static 변수 활용으로 반복 생성 방지

**기대 효과**
- Young Generation 할당 속도 감소
- Minor GC 빈도 감소 (0.3 ops/s → 0.1 ops/s)

### 4. 데이터베이스 커넥션 풀 증설

**우선순위: 중간**

현재 HikariCP max 10은 200 VU 부하에 부족할 수 있음.

**권장 조치**
```yaml
spring:
  datasource:
    hikari:
      maximum-pool-size: 50
      minimum-idle: 20
```

**기대 효과**
- DB 커넥션 대기 시간 감소
- timed-waiting 스레드 감소
- 전체 응답 시간 개선

### 5. 캐싱 전략 도입

**우선순위: 중간**

대기열 순번 조회(COUNT 쿼리)는 캐싱 적용이 유효함.

**권장 조치**
- Redis 캐시 도입
- 대기열 순번 조회 결과를 1~3초간 캐싱
- 폴링 요청의 DB 부하 감소

**기대 효과**
- DB 쿼리 횟수 최대 70% 감소
- 응답 시간 개선
- DB 커넥션 풀 압박 완화

### 6. 인프라 스케일 업/아웃

**우선순위: 장기**

현재 t2.micro (1 vCPU, 1GiB RAM)는 프로덕션 환경에 부족함.

**권장 조치**

**스케일 업**
- EC2: t3.small (2 vCPU, 2GiB RAM) 이상
- RDS: db.t4g.small (2 vCPU, 2GiB RAM) 이상

**스케일 아웃**
- 애플리케이션 서버 2대 이상 (로드 밸런서 적용)
- Read Replica 도입 (폴링 조회 분산)

**기대 효과**
- CPU, 메모리 여유 확보
- 가용성 향상
- 장애 대응 능력 향상

### 7. 모니터링 및 알림 강화

**우선순위: 높음**

GC 메트릭을 지속적으로 모니터링하고 임계값 초과 시 알림을 받음.

**권장 조치**
- GC 일시정지 시간 임계값 설정 (예: 100ms 초과 시 알림)
- Heap 사용률 모니터링 (80% 초과 시 경고)
- timed-waiting 스레드 수 모니터링

**기대 효과**
- 문제 조기 감지
- 사전 대응 가능
- 장애 예방

---

## VII. Lesson Learnt

### 1. 컨테이너 환경에서의 JVM 메모리 설정 주의

**핵심 교훈**

Java 10+ 버전에서는 `-XX:+UseContainerSupport` 옵션이 기본값으로 활성화됨. 이로 인해 `JAVA_OPTS`에서 명시적으로 설정한 JVM 메모리 할당 크기가 무시될 수 있음.

**문제 상황**
- 설정: `-Xmx512m -Xms256m`
- 실제: 컨테이너 메모리 제한에 따라 256MB만 할당됨
- 결과: 의도한 것보다 적은 힙 메모리로 운영됨

**해결 방법**
```bash
# 방법 1: 명시적으로 UseContainerSupport 비활성화
-XX:-UseContainerSupport -Xmx512m

# 방법 2: 컨테이너 메모리 비율 기반 설정 (권장)
-XX:InitialRAMPercentage=50.0 -XX:MaxRAMPercentage=75.0
```

### 2. GC 모니터링의 중요성

**핵심 교훈**

GC는 애플리케이션 성능에 직접적인 영향을 미침. 특히 Major GC는 수백 밀리초의 STW를 유발하여 응답 시간을 급격히 증가시킬 수 있음.

**시사점**
- GC 메트릭을 실시간으로 모니터링해야 함
- Major GC 발생 빈도와 일시정지 시간을 추적해야 함
- GC 로그 분석을 통해 메모리 사용 패턴을 파악해야 함

### 3. 프로덕션 환경에서의 부하 테스트 필요성

**핵심 교훈**

로컬 환경이나 개발 환경에서는 발견하기 어려운 문제가 실제 부하 상황에서 드러남. 200 VU의 지속적인 부하는 메모리, GC, 스레드 등 다양한 병목을 노출시킴.

**시사점**
- 실제 트래픽 패턴을 반영한 부하 테스트가 필수임
- 단순 스파이크 테스트가 아닌 지속 부하 테스트가 중요함
- 피크 시간대를 충분히 길게 유지해야 문제를 발견할 수 있음

### 4. 메모리 사이징의 중요성

**핵심 교훈**

애플리케이션의 예상 부하에 따라 적절한 힙 크기를 설정해야 함. 256MB는 200 VU를 처리하기에 부족했으며, 이는 잦은 GC와 성능 저하로 이어짐.

**시사점**
- 힙 크기는 예상 부하의 1.5~2배로 여유있게 설정
- Young Generation과 Old Generation 비율 조정
- 메모리 프로파일링을 통해 객체 생성 패턴 분석

### 5. 단계적 부하 증가의 유효성

**핵심 교훈**

7단계로 나눈 점진적 부하 증가 패턴은 어느 지점에서 문제가 발생하는지 명확히 파악할 수 있게 함. 급격한 부하 증가보다 단계적 접근이 디버깅에 효과적임.

**시사점**
- 워밍업 단계를 충분히 가져야 함
- 각 단계별로 충분한 시간을 유지해야 함
- 피크 부하를 10분 이상 유지하여 지속 가능성을 검증해야 함

### 6. 다층적 모니터링의 필요성

**핵심 교훈**

단순 HTTP 메트릭만으로는 부족함. JVM 메트릭(GC, Heap, Thread), 인프라 메트릭(CPU, Memory), 애플리케이션 메트릭(DB 커넥션)을 종합적으로 모니터링해야 근본 원인을 파악할 수 있음.

**시사점**
- 그라파나 대시보드에 JVM 메트릭 필수 포함
- 스레드 상태 변화를 실시간으로 추적
- GC 발생과 응답 시간의 상관관계 분석

### 7. DB 대기열의 한계 인식

**핵심 교훈**

DB 기반 대기열은 구현이 간단하지만 높은 부하에서는 한계가 명확함. COUNT 쿼리의 반복 실행은 DB와 커넥션 풀에 부담을 줌.

**시사점**
- 대규모 트래픽에는 Redis 기반 대기열이 적합함
- DB 대기열은 소규모 서비스나 초기 단계에 적합함
- 트래픽 증가 시 아키텍처 전환 계획 수립 필요
